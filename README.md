# ppLasso

In the present era of big data, the volume of health data generated by healthcare providers has significantly increased. As a result, traditional statistical tools for high-dimensional data variable selection have become computationally inefficient. To address this issue, the `ppLasso` R package has been developed, which facilitates efficient variable selection in multi-center data. Our statistical tool outperforms existing methods by a substantial margin, as demonstrated by both simulated studies and real data.

# Introduction

The increasing availability of large-scale datasets collected by national registries has provided extraordinary opportunities for biomedical research. However, as the number of healthcare providers included in these datasets escalates along with the sample size, many commonly used statistical tools for high-dimensional variable selection designed for general purposes become computationally inefficient. This is particularly true when the number of providers is comparable to or exceeds the dimension of risk factors, posing a significant obstacle for scientists to effectively utilize the rich data sources at their disposal.

If researchers aim to adjust the high-dimensional center-level effect in risk models, the parameter space can admit hundreds, or even thousands, of centers. However, this poses a significant challenge for (block) coordinate descent-oriented methods, such as the widely used R packages glmnet or grpreg, as their updates are inherently ergodic. The sweep of the entire parameter space can substantially prolong convergence time, especially with the rapid increase in the number of centers. The problem is compounded when fitting discrete time-to-event models, where the data must be expanded longitudinally into its corresponding binary representation. This expansion results in a dataset several, or even tens, of times larger than the original, causing a dramatic increase in both the memory required and the time to convergence.

Our proposed solution is inspired by Block Ascent Newton and serves as an extension of the (block) coordinate descent algorithm. Specifically, for GLM problems, an iterative procedure updates the center effects and penalized parameters of risk factors. In the outer layer of each iteration, Newton's method is used to update the unpenalized center effects, followed by the subdifferential-based (block) coordinate descent method for updating the coefficients of risk factors using the updated center effect in the inner layer. For discrete survival models, an additional middle layer updates the required estimate of baseline hazard over different time points without data expansion. Due to the unique structure of the data, the Newton method proves to be more efficient than the coordinate descent method in updating the center effect and baseline hazard. As a result, our proposed algorithm can markedly reduce processing time and memory requirements.
