# ppLasso

In the contemporary era of big data, the volume of health data generated by healthcare providers, such as hospitals and dialysis facilities, has experienced a remarkable upsurge. As a result, traditional statistical tools for variable selection in high-dimensional data have encountered challenges in maintaining computational efficiency. In response to this issue, the `ppLasso` R package has been meticulously developed, offering a powerful solution for efficient variable selection in multi-center data. Demonstrating its superiority, our statistical tool outperforms existing methods by a substantial margin, as validated through both simulated studies and real-world data.

# Introduction

The increasing availability of large-scale datasets collected by national registries has opened extraordinary opportunities for biomedical research. However, as the number of healthcare providers  included in these datasets escalates, along with the sample size, many commonly used statistical tools for high-dimensional variable selection designed for general purposes become computationally inefficient. This becomes particularly true when the number of providers is comparable to or exceeds the dimension of risk factors, presenting a significant obstacle for scientists to effectively harness the rich data sources at their disposal.

When researchers aim to adjust the high-dimensional center-level effect in risk models, the parameter space can encompass hundreds, or even thousands, of centers. However, this poses a significant challenge for (block) coordinate descent-oriented methods, such as the widely used R packages `glmnet` or `grpreg`, as their updates are inherently ergodic. The sweep of the entire parameter space can substantially prolong the convergence time, especially with the rapid increase in the number of centers. The problem is further exacerbated when fitting discrete time-to-event models, where the data must be expanded longitudinally into its corresponding binary representation. This expansion results in a dataset several, or even tens, of times larger than the original, causing a dramatic surge in both the required memory and the time to convergence.

Our proposed solution draws inspiration from Block Ascent Newton and serves as an extension of the (block) coordinate descent algorithm. Specifically, for GLM problems, an iterative procedure updates the center effects and penalized parameters of risk factors. In the outer layer of each iteration, Newton's method is used to update the unpenalized center effects, followed by the subdifferential-based (block) coordinate descent method for updating the coefficients of risk factors using the updated center effect in the inner layer. For discrete survival models, an additional middle layer updates the required estimate of the baseline hazard over different time points without data expansion. Due to the unique structure of the data, the Newton method proves to be more efficient than the coordinate descent method in updating the center effect and baseline hazard. As a result, our proposed algorithm can significantly reduce processing time and memory requirements.
